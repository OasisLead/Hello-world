{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OasisLead/Hello-world/blob/main/SD_credit_fraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **SD 201 PROJECT**\n"
      ],
      "metadata": {
        "id": "2XTCke-akjmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "The question we will try to answer in this project is : How to detect credit card frauds ?\n",
        "\n",
        "\n",
        "\n",
        "**Data set**\n",
        "\n",
        "\n",
        "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "This data set contains most of the information that a Bank would have available on a transaction to detect if it is a fraud or not. One of the challenges of fraud detection is that the amount of information that you can gather on a single transaction is clearly limited.\n",
        "\n",
        "This data set is interesting because it is realistic concerning the number of information for each transaction.\n",
        "\n",
        "However, it contains only transactions done during 2 days. We could have more data ( more data is always better), but this quantity is sufficient to train a machine learning classification model and to gather enough correlations to make real inference. Moreover, having a restricted data set in time may be a good thing, because as the market and banking world evolves, data will naturally change, so we can't really count on old data to solve new problems. A realistic algorithm would have to update its parameters as the market evolves, and maybe, use transfer learning to adapt the old model ( trained on old data) to the new data. In this project we won't use transfer learning, and will focus on building a classification model from the 2 days data to detect future frauds.\n",
        "\n",
        "\n",
        "\n",
        "To describe the data set more precisely, it contains only vectors of numbers. Each vector represents a credit card transaction.\n",
        "The different attributes of the vectors are either simple information like time or amount, or they are private informations on the transaction, therefore having no real name ( They are called V1, V2, etc...). This may be a problem in case we want to interpret the models and the data. However, a bank would have access to the hidden labels and could interpret the impact of each feature on the model and on the class.\n",
        "All features except class, time and amount are the result of a PCA transformation.\n",
        "We have two labeled classes, fraud and non-fraud transactions.\n",
        "\n",
        "**Goals**:\n",
        "\n",
        "<ul>\n",
        "<li> Understand the distribution of the data ( The role of each columns, correlations, differences beetween classes) </li>\n",
        "\n",
        "<li> Pre-process the data set (normalization, outliers) and create a 50/50 sub-sample for classification which has as many fraud as non-fraud, and use oversampling to add more non-fraud data (To deal with the unbalanced set, we will also use cost-sensitive learning and compare different cost functions)</li>\n",
        "\n",
        "<li> Choose the classifiers we are going to use, compare the classifiers and decide which one has a higher accuracy ( Choose beetween different classifiers and different parameters for each classifier), avoid overfitting with a validation set </li>\n",
        "\n",
        "</ul>\n",
        "\n",
        "Link of the dataset : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download\n",
        "\n"
      ],
      "metadata": {
        "id": "SEbbUGGOjHLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imported Libraries**"
      ],
      "metadata": {
        "id": "N3mhQyK3jSZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn as sk\n",
        "import numpy as np\n",
        "from sklearn import cluster\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from math import *\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import random as rd"
      ],
      "metadata": {
        "id": "mihZRwRZjQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-processing**"
      ],
      "metadata": {
        "id": "M2DHolqfspq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Si vous avez le fichier en local\n",
        "\n",
        "#df = pd.read_csv(open(\"creditcard.csv\")) #-> Pour lancer la version locale\n",
        "#df.dropna()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lOwx3-QwmP6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Si vous avez le fichier sur votre google drive sur GOOGLE COLAB\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(open(\"/content/drive/MyDrive/creditcard.csv\"))\n",
        "df.dropna()"
      ],
      "metadata": {
        "id": "ukFLwkmCnVi4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "35188659-a0ae-41d7-e64b-5e012b3f44f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Time         V1         V2        V3        V4        V5  \\\n",
              "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
              "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
              "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
              "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
              "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
              "...          ...        ...        ...       ...       ...       ...   \n",
              "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
              "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
              "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
              "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
              "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
              "\n",
              "              V6        V7        V8        V9  ...       V21       V22  \\\n",
              "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
              "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
              "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
              "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
              "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
              "...          ...       ...       ...       ...  ...       ...       ...   \n",
              "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
              "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
              "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
              "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
              "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
              "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
              "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
              "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
              "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
              "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
              "...          ...       ...       ...       ...       ...       ...     ...   \n",
              "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
              "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
              "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
              "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
              "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
              "\n",
              "        Class  \n",
              "0           0  \n",
              "1           0  \n",
              "2           0  \n",
              "3           0  \n",
              "4           0  \n",
              "...       ...  \n",
              "284802      0  \n",
              "284803      0  \n",
              "284804      0  \n",
              "284805      0  \n",
              "284806      0  \n",
              "\n",
              "[284807 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d613bdbf-a007-4f7f-922d-f2a505ff55fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>172786.0</td>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>...</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>172787.0</td>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>24.79</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>...</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>67.88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>-0.240440</td>\n",
              "      <td>0.530483</td>\n",
              "      <td>0.702510</td>\n",
              "      <td>0.689799</td>\n",
              "      <td>-0.377961</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>-0.686180</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>0.392087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.265245</td>\n",
              "      <td>0.800049</td>\n",
              "      <td>-0.163298</td>\n",
              "      <td>0.123205</td>\n",
              "      <td>-0.569159</td>\n",
              "      <td>0.546668</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.104533</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>172792.0</td>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>217.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284807 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d613bdbf-a007-4f7f-922d-f2a505ff55fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d613bdbf-a007-4f7f-922d-f2a505ff55fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d613bdbf-a007-4f7f-922d-f2a505ff55fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
        "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n"
      ],
      "metadata": {
        "id": "Mz35zOKxzpZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c8b61b-0394-4bfe-aefe-461aa1906b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Frauds 99.83 % of the dataset\n",
            "Frauds 0.17 % of the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of the distributions of the columns \"amount\" (Amount of the transaction) and \"time\" ( Date of transaction)**"
      ],
      "metadata": {
        "id": "YIn5kYJU0hO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
        "\n",
        "df_copy = df.copy()\n",
        "df_copy = df_copy.drop(df_copy[df_copy['Amount'] == 0].index)\n",
        "df_copy['Amount'] = df_copy['Amount'].apply(lambda x: log(x))\n",
        "amount_val = df_copy['Amount'].values\n",
        "time_val = df_copy['Time'].values\n",
        "\n",
        "sns.distplot(amount_val, ax=ax[0], color='r')\n",
        "ax[0].set_title('Distribution of log(Transaction Amount)', fontsize=14)\n",
        "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
        "\n",
        "sns.distplot(time_val, ax=ax[1], color='b')\n",
        "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
        "ax[1].set_xlim([min(time_val), max(time_val)])\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RO3oYoQZ0hm4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "745ca156-78f0-4f67-8260-c13aa449ff12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see than the distributions differ not only in variance and mean, but also by a large margin on their maximum and their minimum. To avoid some classification biases (For example the bias that consists in overestimating the impact of one parameter on the class), we will normalize our values in mean and variance.\n",
        "\n",
        "\n",
        "\n",
        "**Correlation matrix**"
      ],
      "metadata": {
        "id": "LVKC4gZt0h8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting each columns distributions for both classes"
      ],
      "metadata": {
        "id": "-GjtPqoMTvux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
        "\n",
        "df_copy_1= df_copy.copy()\n",
        "df_copy_1 = df_copy_1.drop(df_copy_1[df_copy_1['Class']==0].index)\n",
        "\n",
        "\n",
        "df_copy = df_copy.drop(df_copy[df_copy['Class']==1].index)\n",
        "\n",
        "amount_val_fraud = df_copy_1['Amount'].values\n",
        "time_val_fraud = df_copy_1['Time'].values\n",
        "amount_val_non_fraud = df_copy['Amount'].values\n",
        "time_val_non_fraud = df_copy['Time'].values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sns.distplot(amount_val_fraud, ax=ax[0], color='r')\n",
        "ax[0].set_title('Distribution of Transaction amount for frauds', fontsize=14)\n",
        "ax[0].set_xlim([min(time_val), max(time_val)])\n",
        "\n",
        "sns.distplot(amount_val_non_fraud, ax=ax[1], color='b')\n",
        "ax[1].set_title('Distribution of Transaction amount for non frauds', fontsize=14)\n",
        "ax[1].set_xlim([min(time_val), max(time_val)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ylE-yqzbT--q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "10bfa37e-cd3f-4e81-f564-f4dea1fa26b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return n/db/n.sum(), bin_edges\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5D8uRBkigPkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f353623-9435-4bb2-b854-b4a201ec7dd9",
        "_kg_hide-input": true,
        "_uuid": "e2f417c5d7c633a1e3cdfaa78acd6bd77a38400e",
        "id": "yBiYBdIYltgp"
      },
      "outputs": [],
      "source": [
        "f, ax1 = plt.subplots(1, 1, figsize=(24,20))\n",
        "\n",
        "# Entire DataFrame\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
        "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that some variables are strongly correlated, especially to class, amount and time. This is logical since there was already a PCA for the columns V1..V28, as a result, they are not really correlated together but only to other columns\n",
        "\n",
        "Moreover, some columns like V14, V12 and V17 are strongly correlated with the class, meaning that they will probably have a greater impact of the algorithms since they are linear algorithm mostly. ( Some algorithms may function in quadratic realms, but for example, Trees only consider inequalities)\n",
        "\n",
        "\n",
        "It is hard to analyze the meaning of these correlations without having the names of the columns. In addition, we won't suppress any higly correlated column ( The only one is only V2 and Amount and it won't change the algorithm much).\n",
        "\n",
        "**Scaling and subsampling**"
      ],
      "metadata": {
        "id": "mkNy3hdaQDTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since most of our data has already been scaled in the original csv file,\n",
        "# we should scale the columns that are left to scale (Amount and Time)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# RobustScaler is less prone to outliers (Imagine one super high value)\n",
        "\n",
        "rob_scaler = RobustScaler()\n",
        "\n",
        "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "\n",
        "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "\n",
        "scaled_amount = df['scaled_amount']\n",
        "scaled_time = df['scaled_time']\n",
        "\n",
        "\n",
        "\n",
        "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
        "df.insert(0, 'scaled_amount', scaled_amount)\n",
        "df.insert(1, 'scaled_time', scaled_time)\n",
        "\n",
        "# Amount and Time are scaled\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8PYOWsr2tSFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, knowing that there is only 0.17 % of frauds (the data is highly imbalanced), if we keep for the training of our models the whole data, the score of the classifier will be naturally high because he will be biaised to detect non-frauds.\n",
        "We don't want that. We want to specifically detect frauds and their particular structure. To do that, we are going to concatenate two subsets to make our training set. The first set will be the set of all frauds ( of size 492), and the second set the first 492 non frauds."
      ],
      "metadata": {
        "id": "DZmmR9htaV7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMOTE - Tomek Links method:"
      ],
      "metadata": {
        "id": "vu6GhTELd-tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#LÃ  quelques trucs sont dans le dÃ©sordres, genre le removal des outliers est aprÃ¨s etc.. Parceque il dÃ©pend de l'ensemble\n",
        "#qu'on prend\n",
        "## With SMOTE-Tomek Links method which does both undersampling with Tomek Links and oversampling\n",
        "# with SMOTE\n",
        "\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "## With SMOTE-Tomek Links method\n",
        "\n",
        "\n",
        "#\n",
        "#Define X and Y, as well as training, validation and test\n",
        "\n",
        "df_bis = df.copy()\n",
        "\n",
        "df_bis = df_bis.sample(frac=1)\n",
        "\n",
        "X = df_bis.drop('Class', axis=1)\n",
        "y = df_bis['Class']\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "model=RandomForestClassifier(criterion='entropy')\n",
        "# Define SMOTE-Tomek Links\n",
        "resample=SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
        "# Define pipeline\n",
        "pipeline=Pipeline(steps=[('r', resample), ('m', model)])\n",
        "# Define evaluation procedure (here we use Repeated Stratified K-Fold CV)\n",
        "cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# Evaluate model\n",
        "scoring=['accuracy','precision_macro','recall_macro']\n",
        "scores = cross_validate(pipeline, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
        "\n",
        "# summarize performance\n",
        "print('Mean Accuracy: %.4f' % np.mean(scores['test_accuracy']))\n",
        "print('Mean Precision: %.4f' % np.mean(scores['test_precision_macro']))\n",
        "print('Mean Recall: %.4f' % np.mean(scores['test_recall_macro']))"
      ],
      "metadata": {
        "id": "aAkdVbFtd9Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We take a sub set of the whole data\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "fraud_df = df.loc[df['Class'] == 1]\n",
        "#non_fraud_df = df.loc[df['Class']==0].sample(frac=1, random_state = 18)[:492]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
        "\n",
        "new_df\n",
        "\n",
        "new_df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f6NMK_AB4--H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we use the subsample in our correlation\n",
        "\n",
        "f, ax2 = plt.subplots(1, 1, figsize=(24,20))\n",
        "\n",
        "\n",
        "\n",
        "#Balanced data frame\n",
        "sub_sample_corr = new_df.corr()\n",
        "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
        "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C8WmG3eUkmJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Analysis of the correlation matrix and removal of outliers\n",
        "\n",
        "We can now see on the last line of the correlation matrix that some values have a higher (absolute) correlation with the class. This means that they will probably have a bigger influence on the learning algorithm than other classes ( even if for most algorithms non linear relations are also important). We also see that undersampling has changed the structure of the correlations. Indeed, Whereas before because of PCA most vectors where uncorrelated ( And not independant), now, lots are correlated."
      ],
      "metadata": {
        "id": "aOz83xzg4cfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As some features have a more important role in the detection of the class, we don't want the outliers of theses features to influence too much our results. As a consequence, for the three most correlated to the class features, we will remove their outliers."
      ],
      "metadata": {
        "id": "uixnkfikNSqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\n",
        "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
        "print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n",
        "v14_iqr = q75 - q25\n",
        "print('iqr: {}'.format(v14_iqr))\n",
        "\n",
        "v14_cut_off = v14_iqr * 1.5\n",
        "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
        "print('Cut Off: {}'.format(v14_cut_off))\n",
        "print('V14 Lower: {}'.format(v14_lower))\n",
        "print('V14 Upper: {}'.format(v14_upper))\n",
        "\n",
        "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
        "print('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
        "print('V10 outliers:{}'.format(outliers))\n",
        "\n",
        "new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
        "print('----' * 44)\n",
        "\n",
        "# -----> V12 removing outliers from fraud transactions\n",
        "v12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
        "v12_iqr = q75 - q25\n",
        "\n",
        "v12_cut_off = v12_iqr * 1.5\n",
        "v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n",
        "print('V12 Lower: {}'.format(v12_lower))\n",
        "print('V12 Upper: {}'.format(v12_upper))\n",
        "outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n",
        "print('V12 outliers: {}'.format(outliers))\n",
        "print('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
        "new_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\n",
        "print('Number of Instances after outliers removal: {}'.format(len(new_df)))\n",
        "print('----' * 44)\n",
        "\n",
        "\n",
        "# Removing outliers V10 Feature\n",
        "v10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\n",
        "q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
        "v10_iqr = q75 - q25\n",
        "\n",
        "v10_cut_off = v10_iqr * 1.5\n",
        "v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n",
        "print('V10 Lower: {}'.format(v10_lower))\n",
        "print('V10 Upper: {}'.format(v10_upper))\n",
        "outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
        "print('V10 outliers: {}'.format(outliers))\n",
        "print('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
        "new_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n",
        "print('Number of Instances after outliers removal: {}'.format(len(new_df)))"
      ],
      "metadata": {
        "id": "06wg-mGYNrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85ce8738-7599-4b06-a722-5c0ed073599b",
        "_kg_hide-input": true,
        "_uuid": "e3751d88766a982119e522e27a9c0c647f20af85",
        "id": "MWvUL5Wrltgz"
      },
      "outputs": [],
      "source": [
        "X = new_df.drop('Class', axis=1)\n",
        "print(X)\n",
        "y = new_df['Class']\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "288a65b7-8b86-44b1-973d-38dbcfe82bbb",
        "_kg_hide-input": true,
        "_uuid": "fb0a479efaa7147d6702c2c24083f1118621863f",
        "id": "8-saQ4cHltg0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# one part of the data is used for training and the other for testing ( 20% for testing )\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bccd5685-a979-451e-85b3-1cb968523540",
        "_kg_hide-input": true,
        "_uuid": "28f5178089d2d133b9e7478c1c7dc7a1f98aabee",
        "id": "5sn8ypp-ltg2"
      },
      "outputs": [],
      "source": [
        "# Turn the values into an array for feeding the classification algorithms.\n",
        "X_train = X_train.values\n",
        "print(len(X_train))\n",
        "\n",
        "X_test = X_test.values\n",
        "print(len(X_test))\n",
        "\n",
        "y_train = y_train.values\n",
        "print(len(y_train))\n",
        "\n",
        "y_test = y_test.values\n",
        "print(len(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification and results**\n",
        "\n",
        "Now that we have cleaned and processed our data, we will try to classify it using different estimators and comparing the results."
      ],
      "metadata": {
        "id": "kYmbkIiVcGxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7810d0b9-b4e5-4b7f-909b-c127365b167c",
        "_kg_hide-input": true,
        "_uuid": "8dd4ea07fd60973fccabc2d46af28a09b0de9178",
        "id": "YYa715dEltg2"
      },
      "outputs": [],
      "source": [
        "# Here are the 5 classifiers that we will use\n",
        "\n",
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(max_iter = 1000), #max_iter = 1000\n",
        "    \"KNearest\": KNeighborsClassifier(),\n",
        "    \"randomforest\":RandomForestClassifier(),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb37c0f6-9cfe-48b6-92d3-475d5e6767a6",
        "_kg_hide-input": true,
        "_uuid": "fe129af379caccc5428cf1836e6c96bd32e68feb",
        "id": "ptnOhODhltg3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import time\n",
        "\n",
        "for key, classifier in classifiers.items():\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    #training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "\n",
        "\n",
        "    classifier.fit(X_train, y_train)\n",
        "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "\n",
        "    print(classifier.__class__.__name__ + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    #classifier.fit(X_train, y_train)\n",
        "    score2 = classifier.score(X_train,y_train)\n",
        "    score1 = classifier.score(X_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(score2, 2) * 100, \"% accuracy score\")\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a testing score of\", round(score1, 2) * 100, \"% accuracy score\")\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a cross validation training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe very high scores both on testing set and on cross validation or the training set.\n",
        "We see that for the tree and the forest method, the algorithm was able to calculate the \"perfect\" model with 100% accuracy on the test set ( Also, cross validation really is only for score purpose since it does a copy of the model and doesn't change its parameters).\n",
        "This mean that we could have better results on the test set by maybe limiting the \"perfectness\" of the trees to avoid overfitting. This could simply be done by limiting the size of the trees.\n",
        "The problem of this data set is that it is really unbalanced, so we don't have that much data in the end when we consider the fraud case. Since we don't have much data, it is really easy to find the \"perfect model\" on our data, without really gathering any compressed representation of our data.\n",
        "One way to solve this issue would be to change our training set, by adding data with oversampling, and by less reducing data by using a less harsh underfitting algorithm which leaves more data for training (For example, it would be 1/3 - 2/3 instead of half - half).\n",
        "\n",
        "- \"Training score\" represents the score on the test data of the classifier fitted on the whole training data ( 80% of the 984 vectors).\n",
        "\n",
        "- \"Cross validation training score\" uses the technique of cross training to train the data on a part of the training data and testing it on another part ( There are 5 parts).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RDJrENz9YcFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "list_params = [2,3,4,5,6,7,8,9,10,20,50]\n",
        "\n",
        "\n",
        "for max_depth in list_params:\n",
        "\n",
        "    classifier = sk.tree.DecisionTreeClassifier(max_depth = max_depth, random_state=0)\n",
        "\n",
        "    classifier.fit(X_train, y_train)\n",
        "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    score2 = classifier.score(X_train,y_train)\n",
        "    score1 = classifier.score(X_test,y_test)\n",
        "\n",
        "\n",
        "    print(\"Classifiers: \", \"tree of max depth\", max_depth, \"Has a training score of\", round(score2, 2) * 100, \"% accuracy score\")\n",
        "    print(\"Classifiers: \",\"tree of max depth\", max_depth, \"Has a testing score of\", round(score1, 2) * 100, \"% accuracy score\")\n",
        "    print(\"Classifiers: \", \"tree of max depth\", max_depth, \"Has a cross validation training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
        "    dot_data = StringIO()\n",
        "    export_graphviz(classifier, out_file=dot_data,filled=True, rounded=True,\n",
        "                    special_characters=True,class_names=['0','1'])\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "    graph.write_png('lolgameskills.png')\n",
        "    Image(graph.create_png())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "pfghOoxycPfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see on this example a clear example of overfitting. The more we inscrease the precision of our tree on the training score, the less it is precise on the test score.\n",
        "\n",
        "However, we lack a validation set, because, as it is, if we take the best compromise of precision on test set and precision on training set, we have no real test set left as we are biaised with the test set."
      ],
      "metadata": {
        "id": "CnSVujL0d4f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost sensitive learning"
      ],
      "metadata": {
        "id": "U4-xi1bautdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IR = 0.17/99.83 # IR doit valoir le rapport entre le pourcentage de cas de fraude et le pourcentage de cas de non fraude\n",
        "#Pour l'instant, il vaut 1, avec le undersampling qu'on a fait, mais si on fait la mÃ©thode SMOTE et le undersampling, on obtiendra\n",
        "# un data set incomplet dont il faudra rÃ©cuperer les pourcentages respectifs\n",
        "\n",
        "class_weight={0:IR,1:1}\n",
        "\n",
        "classifier = sk.tree.DecisionTreeClassifier(max_depth=5,class_weight=class_weight,random_state=0)\n",
        "\n",
        "(results_df_dt_cost_sensitive, classifier_0, train_df, test_df) = kfold_cv_with_classifier(classifier,\n",
        "                                                                         X, y,\n",
        "                                                                         n_splits=5,\n",
        "                                                                         strategy_name=\"Decision tree - Cost-sensitive\")\n",
        "\n",
        "fig_decision_boundary = plot_decision_boundary(classifier_0, train_df, test_df)"
      ],
      "metadata": {
        "id": "a6YTUtWNus1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Finding the best hyperparameter**\n",
        "\n",
        "To find the best hyperparameter, we are going to use the grid method. It is basically a method that optimises any combination of parameters by testing its particular score.\n",
        "\n",
        "We will be testing different types of penalty function for our algorithms, different kinds of kernels for SVC, and different criteria and sizes for the tree."
      ],
      "metadata": {
        "id": "O_ugxyIecTPh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a1c35773-f4c7-4caf-9911-532784c9eae0",
        "_kg_hide-input": true,
        "_uuid": "d15b1ab16737358806e34c48dc57aa238cf0cfd2",
        "id": "yKJ3yMQNltg4"
      },
      "outputs": [],
      "source": [
        "# Use GridSearchCV to find the best parameters.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# The C parameter is the regulation parameter to avoid overfitting, much like the max depth for the trees\n",
        "\n",
        "# CrÃ©er la liste des diffÃ©rents coefficients de fonction de cout associÃ©s pour chaque classe que l'on va tester\n",
        "# et faire gaffe si ces coefficients sont bien adaptÃ©s Ã  l'algo, ensuite, bien changer la fonction de score sur laquelle on Ã©value\n",
        "# les algorithmes\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "\n",
        "\n",
        "grid_log_reg = GridSearchCV(LogisticRegression(max_iter = 1000), log_reg_params)\n",
        "start_time = time.time()\n",
        "grid_log_reg.fit(X_train, y_train)\n",
        "print(classifier.__class__.__name__ + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "# We automatically get the logistic regression with the best parameters.\n",
        "log_reg = grid_log_reg.best_estimator_\n",
        "\n",
        "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "\n",
        "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
        "start_time = time.time()\n",
        "grid_knears.fit(X_train, y_train)\n",
        "print(classifier.__class__.__name__ + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "# KNears best estimator\n",
        "knears_neighbors = grid_knears.best_estimator_\n",
        "\n",
        "# Support Vector Classifier\n",
        "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
        "grid_svc = GridSearchCV(SVC(), svc_params)\n",
        "start_time = time.time()\n",
        "grid_svc.fit(X_train, y_train)\n",
        "print(classifier.__class__.__name__ + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# SVC best estimator\n",
        "svc = grid_svc.best_estimator_\n",
        "\n",
        "# DecisionTree Classifier\n",
        "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\n",
        "              \"min_samples_leaf\": list(range(5,7,1))}\n",
        "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
        "start_time = time.time()\n",
        "grid_tree.fit(X_train, y_train)\n",
        "print(classifier.__class__.__name__ + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# tree best estimator\n",
        "tree_clf = grid_tree.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f327bcd-335f-4e49-af07-fc4214dbcbdc",
        "_kg_hide-input": true,
        "_uuid": "1b2108bf377b924ed8a6efe580d9e162a132cd9e",
        "id": "YJ5FmTsPltg4"
      },
      "outputs": [],
      "source": [
        "# Overfitting Case\n",
        "\n",
        "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
        "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "\n",
        "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
        "print('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
        "print('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
        "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We weirdly find that some scores have decreased over this optimisation, whereas others have increased."
      ],
      "metadata": {
        "id": "PKMWq1EiYWEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating more deeply our classifiers**\n",
        "\n",
        "In this last part, we are going to compute the ROC curves and the confusion matrix to evaluate our classifiers. As well as their time efficiency for a real case use."
      ],
      "metadata": {
        "id": "UCboIDx-lGxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "780e485a-ea64-48a0-ad97-a7516b047f32",
        "_kg_hide-input": true,
        "_uuid": "fdd59bf2c7a8e61cfb401142570643e8a29cf86b",
        "id": "etBjAkt1ltg8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# Create a DataFrame with all the scores and the classifiers names.\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
        "                             method=\"decision_function\")\n",
        "\n",
        "print(\"log_reg_pred\"+ \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
        "print(\"knears_pred\" + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
        "                             method=\"decision_function\")\n",
        "print(\"svc_pred\" + \"---t %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\n",
        "print(\"tree_pred\" + \"---t %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "57c211c6-e88f-4634-b321-4949df08815d",
        "_kg_hide-input": true,
        "_uuid": "cb2e4715e91e36f2029ef2a5c241991ff162cd9f",
        "id": "Y2jdAMdrltg8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
        "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
        "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
        "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89b0b9b6-ef82-4b69-9517-e89a79696dbb",
        "_kg_hide-input": true,
        "_uuid": "9d57aad23f3f72f3c45bf80b089a65acbce2a9ab",
        "id": "hl5NP8lhltg9"
      },
      "outputs": [],
      "source": [
        "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
        "knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
        "svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
        "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
        "\n",
        "\n",
        "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
        "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
        "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n",
        "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n",
        "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.axis([-0.01, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate', fontsize=16)\n",
        "    plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
        "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
        "                )\n",
        "    plt.legend()\n",
        "\n",
        "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13a7d31c-2586-4946-aaa3-60090cd5680b",
        "_kg_hide-input": true,
        "_uuid": "d0e37500506d1b942431ac5bfabedcfea30275ce",
        "id": "xJcZOP0PlthG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "y_pred_knear = knears_neighbors.predict(X_test)\n",
        "y_pred_svc = svc.predict(X_test)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "\n",
        "\n",
        "log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\n",
        "kneighbors_cf = confusion_matrix(y_test, y_pred_knear)\n",
        "svc_cf = confusion_matrix(y_test, y_pred_svc)\n",
        "tree_cf = confusion_matrix(y_test, y_pred_tree)\n",
        "\n",
        "fig, ax = plt.subplots(2, 2,figsize=(22,12))\n",
        "\n",
        "\n",
        "sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\n",
        "ax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\n",
        "ax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "sns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\n",
        "ax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\n",
        "ax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "sns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\n",
        "ax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\n",
        "ax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "sns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\n",
        "ax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\n",
        "ax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\n",
        "ax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour conclure, nous retrouvons des bons resultats pour nous classifiers, toujours sans vraument comprendre d'oÃ¹ vient le score de 100%.\n",
        "\n",
        "On aurait pu calculer nos scores diffÃ©remment, par exemple en se concentrant particuliÃ¨rement sur le taux de true positive et le taux de false negative qui sont les plus importants en cas de fraude bancaire.\n",
        "\n",
        "De plus, il existe certaines mÃ©thodes spÃ©cifiques aux fraudes bancaires qui permettent de calculer en fonction de la probabilitÃ© assignÃ©e Ã  la fraude, une fonction d'esperance qui calcule le coÃ»t pour la banque d'envoyer des experts aprÃ¨s la dÃ©tÃ©ction pour vÃ©rifier que c'est bien une fraude. C'est une fonction de performance avancÃ©e qui prendrait aussi en compte un flux discret de transaction et un certain nombre d'experts, les \"ressources\" de la banque pour aller chercher les fraudes.\n",
        "\n",
        "Limites de notre mÃ©thode:"
      ],
      "metadata": {
        "id": "crvbfpmCm6FU"
      }
    }
  ]
}