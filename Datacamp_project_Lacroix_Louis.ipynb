{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OasisLead/Hello-world/blob/main/Datacamp_project_Lacroix_Louis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the project is to train machine learning models on a EEG dataset(SVM and KNN, could have used a neural network but it is not very useful in this task since the data is pretty small in size, and also since I used Fourier transform to extract the relevant EEG frequencies, I don't use \"time series\" techniques such as LSTM for example).\n",
        "\n",
        "I first pre-process the EEG data set by choosing relevant channels, removing noise, choosing relevant data and doing a fourier transform to obtain the intensity of the different frequencies (which is what we are interested in when we study a general EEG state, rather than a event related potential).\n",
        "The data set consists of EEG data from different experiments, in which multiple participants were recorded for 40 minutes during tasks that demanded various levels of attention.\n",
        "The EEG data has 3 labels : focused (1st 10 minutes), unfocused (10->20 minutes), drowsed (20->40). We will use these labels for classification.\n",
        "\n",
        "Then after this pre-processing, I did multiple PCA pipelines with 4 sizes of basis, and trained SVM and KNN on them. I actually needed to do PCA if I wanted to algorithm to converge in less than 10 minutes, since the data is pretty multidimensional, and since I wanted a good accuracy.\n",
        "\n",
        "The results show a great accuracy, which is kind of surprising, and I don't have explanations for it. Surpisingly the KNN has a better accuracy than the SVM, while converging faster, this I can't really explain. Moreover, the SVM has a better accuracy with increasing PCA basis.\n",
        "\n",
        "I could have done cross-validation or more detailled analysis, to see if my models are generalizable. The point of this was really to make the pre-processing with fourier transform and make the whole pipeline, not to find the best, fastest, and most generalizable model.\n",
        "One idea for further development is that if you do this analysis in real time in a patient, you want the analysis to be pretty fast, so to improve maybe on model size (smaller model for faster computations) and model complexity, and also the analysis doesn't need to take only 10 seconds of sample but can take like 10 minutes of sample, so gather more data before choosing the actual category by choosing the most frequent label in the last 10 minutes for example.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6sByguFqIfa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hr08JMAFc_"
      },
      "source": [
        "# Librairies and data importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPkK6Q6n_7K-"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import stft\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import sys\n",
        "import pickle\n",
        "#!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "#!pip install keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG4V67CqAXK0"
      },
      "source": [
        "# Connexion to drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KZtSEMmAaGk",
        "outputId": "7292f19f-5f17-4904-a709-95db2e513184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnHKhgn3AfZu"
      },
      "source": [
        "# Loading database files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o66gz-PXAf8c"
      },
      "outputs": [],
      "source": [
        "#create a list of files name\n",
        "data_dir = []\n",
        "\n",
        "# set the dataset's path\n",
        "data_path = './drive/My Drive/Colab Notebooks/EEG Data/'\n",
        "\n",
        "number_of_recordings = 34\n",
        "\n",
        "#set the data dir as the name of all files in the directory\n",
        "data_dir = [\"eeg_record\" + str(k) for k in range(1,number_of_recordings +1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#load data from each files in d[name]\n",
        "d = {}\n",
        "for name in data_dir:\n",
        "    d[name] = loadmat(data_path + name + '.mat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miDfXU1lA8fj"
      },
      "source": [
        "# Extraction of signal in recordings and removing bad channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAL05l9wCjjD"
      },
      "source": [
        "Some variable instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWBU_0wDA9D7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29930b80-057e-40a0-c742-e5e28343eeb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sample for 1 signal is 307200\n"
          ]
        }
      ],
      "source": [
        "# sampling frequency is 128 Hz\n",
        "fs = 128\n",
        "# recording last for 40 min\n",
        "sample_nbr = 128 * 40 * 60\n",
        "#sample_nbr is the number of sample for a given signal\n",
        "\n",
        "#1 recording has multiple signal\n",
        "\n",
        "print('Total number of sample for 1 signal is' , sample_nbr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGVXwUSRCnSA"
      },
      "source": [
        "\n",
        "\n",
        "Separation of useful signal from recordings:\n",
        "\n",
        "I kept only the relevant channels (Neuroscience criterion from the paper of the data set)\n",
        "\n",
        "Creation of the dictionnary that will store data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DCWvFyJCq4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7b673c-dc15-4d27-84e6-2baced7bf6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of a recording is now  (307200, 7)\n"
          ]
        }
      ],
      "source": [
        "#available signals in the loaded database for 1 recording\n",
        "signals = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
        "#signals to use in the loaded database for 1 recording ()\n",
        "useful_signals = ['F7','F3','P7','O1','O2','P8','AF4']\n",
        "\n",
        "#extract index number for useful signals\n",
        "use_signal_inds = []\n",
        "for c in useful_signals:\n",
        "    if c in signals:\n",
        "        use_signal_inds.append(signals.index(c))\n",
        "\n",
        "# create a dictionnary for the useful files data\n",
        "eeg = dict()\n",
        "\n",
        "for name in data_dir:\n",
        "    eeg[name] = dict()\n",
        "    eeg[name] = d[name]['o']['data'][0][0][0:sample_nbr, 3:17]\n",
        "    eeg[name] = eeg[name][:,use_signal_inds]\n",
        "\n",
        "print('shape of a recording is now ' ,eeg['eeg_record1'].shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking data quality and removing incomplete data"
      ],
      "metadata": {
        "id": "U95gmM3BwW2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing incomplete channels\n",
        "for name in data_dir:\n",
        "  data_time = len(eeg[name])/128/60\n",
        "  print('Name : ',name,', Data time:',data_time,'minutes ')\n",
        "\n",
        "##Delete the bad channels from data_dir\n",
        "for record in [\"eeg_record28\", \"eeg_record6\", \"eeg_record16\"]:\n",
        "    if record in data_dir:\n",
        "        data_dir.remove(record)\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "for name in data_dir:\n",
        "  data_time = len(eeg[name])/128/60\n",
        "  print('Name : ',name,', Data time:',data_time,'minutes ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHt32i0qZtDW",
        "outputId": "6e8e5175-3c72-485b-97b3-20a1c9f579ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name :  eeg_record1 , Data time: 40.0 minutes \n",
            "Name :  eeg_record2 , Data time: 40.0 minutes \n",
            "Name :  eeg_record3 , Data time: 40.0 minutes \n",
            "Name :  eeg_record4 , Data time: 40.0 minutes \n",
            "Name :  eeg_record5 , Data time: 40.0 minutes \n",
            "Name :  eeg_record6 , Data time: 37.59791666666667 minutes \n",
            "Name :  eeg_record7 , Data time: 40.0 minutes \n",
            "Name :  eeg_record8 , Data time: 40.0 minutes \n",
            "Name :  eeg_record9 , Data time: 40.0 minutes \n",
            "Name :  eeg_record10 , Data time: 40.0 minutes \n",
            "Name :  eeg_record11 , Data time: 40.0 minutes \n",
            "Name :  eeg_record12 , Data time: 40.0 minutes \n",
            "Name :  eeg_record13 , Data time: 40.0 minutes \n",
            "Name :  eeg_record14 , Data time: 40.0 minutes \n",
            "Name :  eeg_record15 , Data time: 40.0 minutes \n",
            "Name :  eeg_record16 , Data time: 30.5171875 minutes \n",
            "Name :  eeg_record17 , Data time: 40.0 minutes \n",
            "Name :  eeg_record18 , Data time: 40.0 minutes \n",
            "Name :  eeg_record19 , Data time: 40.0 minutes \n",
            "Name :  eeg_record20 , Data time: 40.0 minutes \n",
            "Name :  eeg_record21 , Data time: 40.0 minutes \n",
            "Name :  eeg_record22 , Data time: 40.0 minutes \n",
            "Name :  eeg_record23 , Data time: 40.0 minutes \n",
            "Name :  eeg_record24 , Data time: 40.0 minutes \n",
            "Name :  eeg_record25 , Data time: 40.0 minutes \n",
            "Name :  eeg_record26 , Data time: 40.0 minutes \n",
            "Name :  eeg_record27 , Data time: 40.0 minutes \n",
            "Name :  eeg_record28 , Data time: 27.934895833333332 minutes \n",
            "Name :  eeg_record29 , Data time: 40.0 minutes \n",
            "Name :  eeg_record30 , Data time: 40.0 minutes \n",
            "Name :  eeg_record31 , Data time: 40.0 minutes \n",
            "Name :  eeg_record32 , Data time: 40.0 minutes \n",
            "Name :  eeg_record33 , Data time: 40.0 minutes \n",
            "Name :  eeg_record34 , Data time: 40.0 minutes \n",
            "-------------------------------------------------------------------------------\n",
            "Name :  eeg_record1 , Data time: 40.0 minutes \n",
            "Name :  eeg_record2 , Data time: 40.0 minutes \n",
            "Name :  eeg_record3 , Data time: 40.0 minutes \n",
            "Name :  eeg_record4 , Data time: 40.0 minutes \n",
            "Name :  eeg_record5 , Data time: 40.0 minutes \n",
            "Name :  eeg_record7 , Data time: 40.0 minutes \n",
            "Name :  eeg_record8 , Data time: 40.0 minutes \n",
            "Name :  eeg_record9 , Data time: 40.0 minutes \n",
            "Name :  eeg_record10 , Data time: 40.0 minutes \n",
            "Name :  eeg_record11 , Data time: 40.0 minutes \n",
            "Name :  eeg_record12 , Data time: 40.0 minutes \n",
            "Name :  eeg_record13 , Data time: 40.0 minutes \n",
            "Name :  eeg_record14 , Data time: 40.0 minutes \n",
            "Name :  eeg_record15 , Data time: 40.0 minutes \n",
            "Name :  eeg_record17 , Data time: 40.0 minutes \n",
            "Name :  eeg_record18 , Data time: 40.0 minutes \n",
            "Name :  eeg_record19 , Data time: 40.0 minutes \n",
            "Name :  eeg_record20 , Data time: 40.0 minutes \n",
            "Name :  eeg_record21 , Data time: 40.0 minutes \n",
            "Name :  eeg_record22 , Data time: 40.0 minutes \n",
            "Name :  eeg_record23 , Data time: 40.0 minutes \n",
            "Name :  eeg_record24 , Data time: 40.0 minutes \n",
            "Name :  eeg_record25 , Data time: 40.0 minutes \n",
            "Name :  eeg_record26 , Data time: 40.0 minutes \n",
            "Name :  eeg_record27 , Data time: 40.0 minutes \n",
            "Name :  eeg_record29 , Data time: 40.0 minutes \n",
            "Name :  eeg_record30 , Data time: 40.0 minutes \n",
            "Name :  eeg_record31 , Data time: 40.0 minutes \n",
            "Name :  eeg_record32 , Data time: 40.0 minutes \n",
            "Name :  eeg_record33 , Data time: 40.0 minutes \n",
            "Name :  eeg_record34 , Data time: 40.0 minutes \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mumZZkNiEZLp"
      },
      "source": [
        "# Defining feature extraction methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Smoothing function to remove noise"
      ],
      "metadata": {
        "id": "8MbzI9wiwb9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zkh2V_mEdIw"
      },
      "outputs": [],
      "source": [
        "#To clean the EEG data\n",
        "\n",
        "def moving_average_smooth(interval, windowsize):\n",
        "    window = np.ones(int(windowsize)) / float(windowsize)\n",
        "    re = np.convolve(interval, window, 'same') # convolve(a,v): return discrete, linear convolution of a and v.\n",
        "    return re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCbRH9_HEoUJ"
      },
      "outputs": [],
      "source": [
        "#Spectrogram function = the square of the STFT amplitudes\n",
        "func = lambda x:(np.abs(x))**2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the feature_extraction function to do:\n",
        "- Short time Fourier transform over a time window to get the power of the different frequencies in a short term range and store them as the relevant values\n",
        "- Restrict frequency range\n",
        "- Smooth the data and remove noise\n",
        "- Label the data and store the labels"
      ],
      "metadata": {
        "id": "54bZOAFmwqK-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RooHejkdEpoA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def feature_extraction(input_data,window_size):\n",
        "    feature_list = []\n",
        "    current_time = 0\n",
        "    label = list()\n",
        "\n",
        "    ## FEATURE\n",
        "    #for each signal in the recording\n",
        "    for i in range(7):\n",
        "\n",
        "        #calculate the short time fourrier transformation\n",
        "        eeg_feq = stft(input_data[:,i],128,'blackman')\n",
        "\n",
        "        sample_freq = eeg_feq[0]  # Array of sample frequencies.\n",
        "        segment_time = eeg_feq[1] # Array of segment times.\n",
        "        stft_result = eeg_feq[2]  # STFT of `x`\n",
        "\n",
        "        #restrict the frequency range, specific to EEG signal to remove noise\n",
        "        restricted_stft_result = stft_result[1:37,:] # select only frequency between 0.5 and 18Hz with a 0.5 step\n",
        "\n",
        "        # for each frequency, smoothe the  amplitude signal using a 15 s-running average.\n",
        "        for j in range(36):\n",
        "            current = np.apply_along_axis(func, axis=0, arr = restricted_stft_result[j,:] ) #apply square to STFT applitude\n",
        "            feature = moving_average_smooth(current,window_size*128) # apply average smooth to squared stft amplitude\n",
        "            feature_list.append(feature) # add the result to the feature_list\n",
        "\n",
        "    ## LABEL\n",
        "    data_time = len(input_data)/128/60\n",
        "    # 3 labels : focused -> 0 | unfocused -> 1 | drowsed -> 2\n",
        "    for m in range(len(segment_time)):\n",
        "        current_time += data_time/len(segment_time)  # 40 minutes / 2401\n",
        "        if current_time < 10:\n",
        "            label.append(0) #focused\n",
        "        elif 10 < current_time < 20:\n",
        "            label.append(1) #unfocused\n",
        "        else:\n",
        "            label.append(2) #drowsed\n",
        "\n",
        "\n",
        "    feature_list = np.array(feature_list)\n",
        "    label = np.array(label)\n",
        "    #print(feature_list.shape); # (252, 2401) 252 = 7 * 36\n",
        "\n",
        "    return feature_list, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling / Averaging pre-processing"
      ],
      "metadata": {
        "id": "SD2LX3RgxKNr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9vgPDEAFF00"
      },
      "outputs": [],
      "source": [
        "def minmaxscaler_dataframe_train(feature_list):\n",
        "    scaler_list = list()\n",
        "    scaled_feature_list = list()\n",
        "\n",
        "    #for data received from an stft\n",
        "    for i in range(len(feature_list)): #operation done 2401 times\n",
        "        scaler = MinMaxScaler()\n",
        "        x = np.array(feature_list[i]).reshape(-1,1) #set the 252 sample list as a column vector\n",
        "        x = scaler.fit_transform(x)                 #x has 252 lines and 1 column, contains values form 0 to 1\n",
        "        scaled_feature_list.append(x.reshape(-1))   #add to the list x as a single line\n",
        "        scaler_list.append(scaler)\n",
        "\n",
        "    scaled_feature_list = np.array(scaled_feature_list)\n",
        "    scaler_list = np.array(scaler_list)\n",
        "\n",
        "    return scaled_feature_list, scaler_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final pre-processing function pipeline using previously made functions"
      ],
      "metadata": {
        "id": "AFJlrkzIxndR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHC5282UFSJU"
      },
      "outputs": [],
      "source": [
        "def data_extraction_scaling(dirname,window_size) :\n",
        "\n",
        "  #create unvalid 252 vector\n",
        "  feature_list = np.empty([252, 1])\n",
        "  label_list = []\n",
        "\n",
        "  #define feature_list as a array\n",
        "  feature_list = np.array(feature_list)\n",
        "\n",
        "  for name in data_dir:\n",
        "    #apply feature_extraction\n",
        "    feature, label = feature_extraction(eeg[name],window_size)\n",
        "    label_list = np.append(label_list, label)\n",
        "\n",
        "    #apply minMaxScaler\n",
        "    scaled_feature, scaler = minmaxscaler_dataframe_train(feature)\n",
        "\n",
        "    #add extracted feature to the list\n",
        "    feature_list = np.append(feature_list, scaled_feature, axis = 1)\n",
        "    #reshape feature list to have vectors of 252 elements\n",
        "    feature_list = feature_list.reshape(252,-1)\n",
        "\n",
        "  #remove the first unvalid 252 vector\n",
        "  feature_list = feature_list[ :, 1: ];\n",
        "\n",
        "  #define label_list as a array\n",
        "  label_list = np.array(label_list).reshape(-1,1 )\n",
        "\n",
        "  return feature_list.transpose(), label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elGuZPsDGi50"
      },
      "source": [
        "# Feature extraction tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPzJreFMP17Y"
      },
      "source": [
        "Test of the shape of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghnM-MTpGkIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507bda19-03f6-4052-aff4-32bad214ad5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a given recording the feature format is (252, 2401)\n",
            "For the label associated we have (2401,)\n"
          ]
        }
      ],
      "source": [
        "test1, test2 = feature_extraction(eeg['eeg_record1'],15)\n",
        "print('For a given recording the feature format is' ,test1.shape)\n",
        "print('For the label associated we have',test2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o0-nkhzP53y"
      },
      "source": [
        "Test after minmaxscaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOpx66IrP8HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f113a2bc-6222-42ab-e4b3-ea8aa60dd585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a given signal the scaled feature format is  (252, 2401)\n",
            "The scaler list has the format  (252,)\n"
          ]
        }
      ],
      "source": [
        "test3, test4 = minmaxscaler_dataframe_train(test1)\n",
        "print('For a given signal the scaled feature format is ', test3.shape)\n",
        "print('The scaler list has the format ', test4.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690uCxv5KeuE"
      },
      "source": [
        "# Final data extraction and main pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRUquSCzKlvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa4c383-1d28-49ef-f2bb-9a9409eb2fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature_list has a shape of  (74431, 252)\n",
            "Label_list has a shape of (74431, 1)\n",
            "\n",
            "\rOne element of the feature_list has a len of  252\n",
            "One element of the feature_list has a shape of  (252,)\n",
            "\n",
            "\rOne element of the label_list has a len of 1\n",
            "One element of the label_list has a shape of  (1,)\n"
          ]
        }
      ],
      "source": [
        "feature_list, label_list = data_extraction_scaling(data_dir, 15)\n",
        "\n",
        "print('Feature_list has a shape of ', feature_list.shape)\n",
        "print('Label_list has a shape of', label_list.shape)\n",
        "\n",
        "print('\\n\\rOne element of the feature_list has a len of ', len(feature_list[0]))\n",
        "print('One element of the feature_list has a shape of ', feature_list[0].shape)\n",
        "\n",
        "print('\\n\\rOne element of the label_list has a len of', len(label_list[0]))\n",
        "print('One element of the label_list has a shape of ', label_list[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNXVcSIy7BOy"
      },
      "source": [
        "# Neural network data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Zs15iCJqfx"
      },
      "source": [
        "### Creating Training and Test set, with different PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating 4 PCA models of respectively 4, 10, 20 and 100 dimensions"
      ],
      "metadata": {
        "id": "j2N4BehayCd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# feature_list.shape -> (74431, 252)\n",
        "# label_list.shape -> (74431, 1)\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "features_reduced = pca.fit_transform(feature_list)\n",
        "\n",
        "pca1 = PCA(n_components=10)\n",
        "features_reduced1 = pca1.fit_transform(feature_list)\n",
        "\n",
        "pca2 = PCA(n_components=20)\n",
        "features_reduced2 = pca2.fit_transform(feature_list)\n",
        "\n",
        "pca3 = PCA(n_components=100)\n",
        "features_reduced3 = pca3.fit_transform(feature_list)"
      ],
      "metadata": {
        "id": "MeIzhAq1YruQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 4 PCA models to make 4 training-test data sets"
      ],
      "metadata": {
        "id": "bhzwhgiXych7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpSFyDOw7Bkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a71400f-8e55-4da7-e7a1-65ce1d399c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input feature shapes: (59544, 4) (14887, 4)\n",
            "Label output shape: (59544, 1) (14887, 1)\n",
            "Input feature shapes: (59544, 10) (14887, 10)\n",
            "Label output shape: (59544, 1) (14887, 1)\n",
            "Input feature shapes: (59544, 20) (14887, 20)\n",
            "Label output shape: (59544, 1) (14887, 1)\n",
            "Input feature shapes: (59544, 100) (14887, 100)\n",
            "Label output shape: (59544, 1) (14887, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train_PCA, X_test_PCA, Y_train_PCA, Y_test_PCA = train_test_split(np.array(features_reduced), np.array(label_list), test_size=0.2, random_state=42)\n",
        "print('Input feature shapes:',X_train_PCA.shape, X_test_PCA.shape)\n",
        "print('Label output shape:',Y_train_PCA.shape, Y_test_PCA.shape)\n",
        "\n",
        "X_train_PCA1, X_test_PCA1, Y_train_PCA1, Y_test_PCA1 = train_test_split(np.array(features_reduced1), np.array(label_list), test_size=0.2, random_state=42)\n",
        "print('Input feature shapes:',X_train_PCA1.shape, X_test_PCA1.shape)\n",
        "print('Label output shape:',Y_train_PCA1.shape, Y_test_PCA1.shape)\n",
        "\n",
        "X_train_PCA2, X_test_PCA2, Y_train_PCA2, Y_test_PCA2 = train_test_split(np.array(features_reduced2), np.array(label_list), test_size=0.2, random_state=42)\n",
        "print('Input feature shapes:',X_train_PCA2.shape, X_test_PCA2.shape)\n",
        "print('Label output shape:',Y_train_PCA2.shape, Y_test_PCA2.shape)\n",
        "\n",
        "X_train_PCA3, X_test_PCA3, Y_train_PCA3, Y_test_PCA3 = train_test_split(np.array(features_reduced3), np.array(label_list), test_size=0.2, random_state=42)\n",
        "print('Input feature shapes:',X_train_PCA3.shape, X_test_PCA3.shape)\n",
        "print('Label output shape:',Y_train_PCA3.shape, Y_test_PCA3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMgwkMgzZvzp"
      },
      "source": [
        "# Neural network training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "wZ-FjbOfeIpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Creating different models for each PCA pipeline\n",
        "\n",
        "2) Training the Models\n",
        "\n",
        "3) Getting the prediction of the test set\n",
        "\n",
        "4) Printing accuracy on test set\n"
      ],
      "metadata": {
        "id": "6OEMt6IIyqN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier9 = LinearSVC(C=1, random_state=0)\n",
        "\n",
        "classifier0 = OneVsRestClassifier(classifier9,n_jobs=-1)\n",
        "classifier1 = OneVsRestClassifier(classifier9,n_jobs=-1)\n",
        "classifier2 = OneVsRestClassifier(classifier9,n_jobs=-1)\n",
        "classifier3 = OneVsRestClassifier(classifier9,n_jobs=-1)\n",
        "\n",
        "# classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
        "classifier0.fit(X_train_PCA, Y_train_PCA)\n",
        "classifier1.fit(X_train_PCA1, Y_train_PCA1)\n",
        "classifier2.fit(X_train_PCA2, Y_train_PCA2)\n",
        "classifier3.fit(X_train_PCA3, Y_train_PCA3)\n",
        "#Prediction for the Test set\n",
        "Y_pred_svm_PCA0 = classifier0.predict(X_test_PCA)\n",
        "Y_pred_svm_PCA1 = classifier1.predict(X_test_PCA1)\n",
        "Y_pred_svm_PCA2 = classifier2.predict(X_test_PCA2)\n",
        "Y_pred_svm_PCA3 = classifier3.predict(X_test_PCA3)\n",
        "\n",
        "print('Accuracy score PCA 4: ',accuracy_score(Y_pred_svm_PCA0, Y_test_PCA))\n",
        "print('Accuracy score PCA 10: ',accuracy_score(Y_pred_svm_PCA1, Y_test_PCA1))\n",
        "print('Accuracy score PCA 20: ',accuracy_score(Y_pred_svm_PCA2, Y_test_PCA2))\n",
        "print('Accuracy score PCA 100: ',accuracy_score(Y_pred_svm_PCA3, Y_test_PCA3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOdpeJ3NUZ02",
        "outputId": "59c9dae0-0fd8-42a3-8b75-d30378a5f3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score PCA 4:  0.7481695438973601\n",
            "Accuracy score PCA 4:  0.8109760193457379\n",
            "Accuracy score PCA 4:  0.8559817290253241\n",
            "Accuracy score PCA 4:  0.9717874655739908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I focus only on PCA, since if I don't it takes way too many time to fit the classifier"
      ],
      "metadata": {
        "id": "wI8gE4jWeBFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit du  Training set\n",
        "# classifier1 = SVC(kernel = 'linear', random_state = 0)\n",
        "#classifier1 = LinearSVC(C=1, random_state=0)\n",
        "#classifier = OneVsRestClassifier(classifier1,n_jobs=-1)\n",
        "\n",
        "# classifier = svm.SVC(kernel='linear') # Linear Kernel\n",
        "#classifier.fit(X_train, Y_train)\n",
        "#Prediction for the Test set\n",
        "#Y_pred_svm = classifier.predict(X_test)\n",
        "\n",
        "#print('Accuracy score: ',accuracy_score(Y_pred_svm, Y_test))#  classifier.score(X_test,Y_test)\n",
        "# from sklearn.metrics import classification_report\n",
        "# print(classification_report(Y_test, Y_pred_svm))"
      ],
      "metadata": {
        "id": "Yn0Dp8hLMKAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN"
      ],
      "metadata": {
        "id": "WCLkKFCFeQZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Creating different models for each PCA pipeline\n",
        "\n",
        "2) Training the Models\n",
        "\n",
        "3) Printing accuracy on test set"
      ],
      "metadata": {
        "id": "vxDikDiD0hBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "estimator = KNeighborsClassifier()\n",
        "estimator1 = KNeighborsClassifier()\n",
        "estimator2 = KNeighborsClassifier()\n",
        "estimator3 = KNeighborsClassifier()\n",
        "\n",
        "estimator.fit(X_train_PCA, Y_train_PCA)\n",
        "estimator1.fit(X_train_PCA1, Y_train_PCA1)\n",
        "estimator2.fit(X_train_PCA2, Y_train_PCA2)\n",
        "estimator3.fit(X_train_PCA3, Y_train_PCA3)\n",
        "\n",
        "# print(y_valid)\n",
        "print('Accuracy score: ', estimator.score(X_test_PCA, Y_test_PCA))\n",
        "print('Accuracy score: ', estimator1.score(X_test_PCA1, Y_test_PCA1))\n",
        "print('Accuracy score: ', estimator2.score(X_test_PCA2, Y_test_PCA2))\n",
        "print('Accuracy score: ', estimator3.score(X_test_PCA3, Y_test_PCA3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzd3-XgdMbPp",
        "outputId": "a9a10918-3185-4509-d9fe-19d5c4e7455c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score:  0.999126754886814\n",
            "Accuracy score:  0.9989252367837711\n",
            "Accuracy score:  0.9990595821857997\n",
            "Accuracy score:  0.999126754886814\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}